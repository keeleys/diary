<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta http-equiv="X-UA-Compatible" content="ie=edge">
	<title>KEELEYS</title>
	<meta charset="utf-8">
	<!-- 引入配置文件 -->
	
<link rel="stylesheet" href="/diary/css/main.css">

	<link rel="stylesheet"
      href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.16.2/build/styles/default.min.css">
<meta name="generator" content="Hexo 4.2.1"></head>
<body>
	<div id="main">
		<!-- 引入导航 -->
		<header>
			<div class="title">
    <a href="/diary/">KEELEYS</a>
</div>
<ul class="links">
    
    
    <li class="link">
        <a href="https://github.com/keeleys" target="_blank">Github</a>
    </li>
    
    <li class="link">
        <a href="mailto:keeley.jun@qq.com" target="_blank">Email</a>
    </li>
    
</ul>

		</header>
		<!-- 引入正文 -->
		<div id="content">
			<div id="post-header">
	<h1 id="post-author">python安装虚拟环境和一些例子分享</h1>
	<div id="post-detail">
		<span id="post-date">2016 / 01 / 16</span>
		<span id="post-tags">
			
				<span id="post-tag">python</span>
			
		</span>
	</div>
</div>

<div id="article">
	<h2 id="安装virtualenv"><a href="#安装virtualenv" class="headerlink" title="安装virtualenv"></a>安装virtualenv</h2><p>python用pip安装的包和类库都是直接安装在全局环境的，这做法就跟java把你用过的所有jar包都放在系统path一样，长期工作下去累计的类库越来越多，交叉依赖，版本冲突啥的随之而来。<br><code>virtualenv</code>就是为了解决这个问题的，在每个新项目都使用一个不同的虚拟环境，项目中所有的包都安装在这个虚拟环境之下，和全局无关，使用起来库结构简洁清爽。</p>
<a id="more"></a>

<pre><code class="python">sudo pip install virtualenv</code></pre>
<p>新建安放虚拟文件的目录pv，在目录里面新建python虚拟环境,环境名为pv</p>
<pre><code class="bash">mkdir pv
cd pv
virtualenv --no-site-packages pv</code></pre>
<p>进入到pv虚拟环境</p>
<pre><code class="bash">source pv/bin/activate</code></pre>
<p>退出</p>
<pre><code class="bash">deactivate</code></pre>
<h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><pre><code class="bash">导出:
pip freeze &gt; requirements.txt   # 导出项目依赖的包到requirements.txt文件
批量安装:
pip install -r requirements.txt  # 从依赖文件,批量安装包.</code></pre>
<h2 id="做接口测试"><a href="#做接口测试" class="headerlink" title="做接口测试"></a>做接口测试</h2><pre><code class="python">#!/usr/bin/env python
# -*- coding: utf-8 -*-

import requests

url = &quot;http://localhost:8080/ESB/INFORMANAGE/INFORMANAGESERVICE/inforrelease&quot;
header={
  &#39;content-type&#39;:&#39;application/json; charset=utf-8&#39;,
  &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36&#39;
}
param= {
        &quot;msghead&quot;: {
            &quot;authcode&quot;: &quot;123&quot;,
            &quot;devno&quot;: &quot;123&quot;,
            &quot;reqsn&quot;: &quot;123&quot;,
            &quot;servicename&quot;: &quot;inforrelease&quot;,
            &quot;signData&quot;: &quot;123&quot;,
            &quot;tranchannel&quot;: &quot;123&quot;,
            &quot;trandatetime&quot;: &quot;1923&quot;,
            &quot;version&quot;: &quot;v1.0&quot;
        },
        &quot;msgreq&quot;: {
            &quot;content&quot;: &quot;123&quot;,
            &quot;endTime&quot;: &quot;123&quot;,
            &quot;hasRead&quot;: &quot;123&quot;,
            &quot;ids&quot;: [&quot;123&quot;],
            &quot;imglist&quot;: [&quot;123&quot;],
            &quot;shouldPay&quot;: &quot;123&quot;,
            &quot;startTime&quot;: &quot;123&quot;,
            &quot;title&quot;: &quot;123&quot;,
            &quot;type&quot;: &quot;123&quot;,
            &quot;userId&quot;: &quot;123&quot;
        }
    }
s = requests.post(url,json=param,headers = header)
print s.text.encode(&#39;utf-8&#39;)</code></pre>
<h2 id="做爬虫"><a href="#做爬虫" class="headerlink" title="做爬虫"></a>做爬虫</h2><pre><code class="python">#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Created by tianjun on 16/2/21

import requests
from bs4 import BeautifulSoup
import blog
begin_url = &quot;http://www.omgif.net/&quot;
headers = {
    &quot;Referer&quot;: &quot;www.omgif.net&quot;,
    &#39;Connection&#39;:&#39;keep-alive&#39;,
    &#39;Host&#39;:&#39;www.omgif.net&#39;,
    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36&quot;
}


def parse(url):
    s = requests.get(url, timeout=5,headers=headers)
    bs = BeautifulSoup(s.text, &quot;lxml&quot;)
    for contain in bs.find_all(class_=&quot;post-container&quot;):
        b = blog.blog()
        b.type=&quot;2&quot;
        b.site = &quot;omgif&quot;
        b.content_id = contain.contents[1][&quot;id&quot;]
        if contain.h2 is not None:
            b.title = contain.h2.a.string.encode(&#39;utf-8&#39;)
            b.detail = contain.h2.a[&quot;href&quot;]
        imgContent = contain.img
        b.img = imgContent[&quot;src&quot;]
        b.mp4 = imgContent[&quot;data-gif&quot;]
        if not b.hasContent():
            b.save()
            print &quot;新增一条记录&quot;,b.content_id
        else:
            break
    return bs.find(class_=&quot;archive-nav&quot;).a[&#39;href&#39;]


def init():
    url = begin_url;
    for i in range(0, 10):
        url = parse(url)
        print url

if __name__ == &#39;__main__&#39;:
    init()
</code></pre>
<pre><code class="python">import urllib
import urllib2
cookies = &quot;lzstat_uv=36646143701674066401|3239393; JSESSIONID=AA3A6FEE4AF7E7878218583316BBC9B4; l=Ap6eJnEdnxIiW7Ybwfnsex2lbj7gDmLb; visitState=pc; CNZZDATA30063739=cnzz_eid%3D2009472116-1453958417-%26ntime%3D1453982370; CNZZDATA30073553=cnzz_eid%3D51683217-1453956930-%26ntime%3D1453981617; lzstat_ss=978823405_1_1454014976_3239393; MIIEE_JTOKEN=02978316089138388698541186051443227222067695111036&quot;;
url = &quot;http://www.miiee.com/interface/dresslist.htm?_=04511898399796337&quot;;
values = {&#39;n&#39; : &#39;1&#39;,    
          &#39;p&#39; : &#39;1&#39;,     
          &#39;jtoken&#39; : &#39;02978316089138388698541186051443227222067695111036&#39; }    
headers = { &#39;Cookie&#39; : cookies }
data = urllib.urlencode(values) 
req = urllib2.Request(url, data,headers)
response = urllib2.urlopen(req) 
the_page = response.read();
print the_page
print response.info()
print response.geturl()</code></pre>
<h2 id="一些技巧"><a href="#一些技巧" class="headerlink" title="一些技巧"></a>一些技巧</h2><pre><code class="python">利用map()函数，把用户输入的不规范的英文名字，变为首字母大写，其他小写的规范名字。
输入：[&#39;adam&#39;, &#39;LISA&#39;, &#39;barT&#39;]，输出：[&#39;Adam&#39;, &#39;Lisa&#39;, &#39;Bart&#39;]：


def normalize(name):
    return name[0].upper() +name[1:].lower()

L1 = [&#39;adam&#39;, &#39;LISA&#39;, &#39;barT&#39;]
L2 = list(map(normalize, L1))
print(L2)</code></pre>
<pre><code class="python">from functools import reduce

def prod(L):
    return reduce(lambda x,y: x*y,L)

print(&#39;3 * 5 * 7 * 9 =&#39;, prod([3, 5, 7, 9]))</code></pre>
<h2 id="常用类库"><a href="#常用类库" class="headerlink" title="常用类库"></a>常用类库</h2><ul>
<li><code>beautifulsoup4</code> <a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/" target="_blank" rel="noopener">http://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/</a></li>
<li><code>pyquery</code> <a href="https://github.com/gawel/pyquery" target="_blank" rel="noopener">https://github.com/gawel/pyquery</a></li>
<li><code>requests</code> <a href="http://docs.python-requests.org/en/master/" target="_blank" rel="noopener">http://docs.python-requests.org/en/master/</a></li>
<li><code>MySQL-python</code></li>
<li><code>Scrapy</code><a href="http://doc.scrapy.org" target="_blank" rel="noopener">http://doc.scrapy.org</a></li>
<li><code>Flask</code><a href="http://docs.jinkan.org/docs/flask/" target="_blank" rel="noopener">http://docs.jinkan.org/docs/flask/</a>,<a href="http://dormousehole.readthedocs.org/en/latest/index.html" target="_blank" rel="noopener">SQLAlchemy</a></li>
</ul>

</div>

		</div>
		<footer>
			<div class="seq20"></div>
		</footer>
	</div>
	<!-- 引入 js 文件 -->
	
<script src="/diary/js/main.js"></script>

	<script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.16.2/build/highlight.min.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
</body>
</html>